<!DOCTYPE html>

<html>

<head lang="en">
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>CompAgent</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" href="./files/bootstrap.min.css">
    <link rel="stylesheet" href="./files/font-awesome.min.css">
    <link rel="stylesheet" href="./files/codemirror.min.css">
    <link rel="stylesheet" href="./files/app.css">




</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-20 text-center">
                <br></br>
                Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation<br>
            </h1>
            <hr style="margin-top:0px">
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://scholar.google.com/citations?user=x_-kOjoAAAAJ&hl=zh-CN/" style="font-size: 16px;">
                            Zhenyu Wang
                        </a>
                        <sup>1</sup>
                    </li>
                    <li>
                        <a href="https://xieenze.github.io/" style="font-size: 16px;">
                            Enze Xie
                        </a>
                        <sup>2</sup>
                    </li>
                    <li>
                        <a style="font-size: 16px;">
                            Aoxue Li 
                        </a>
                        <sup>2</sup>
                    </li>
                    <li>
                        <a href="https://zhongdao.github.io/" style="font-size: 16px;">
                            Zhongdao Wang
                        </a>
                        <sup>2</sup>
                    </li>
                    <li>
                        <a href="https://xh-liu.github.io/" style="font-size: 16px;">
                            Xihui Liu
                        </a>
                        <sup>3</sup>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=XboZC1AAAAAJ&hl=en/" style="font-size: 16px;">
                            Zhenguo Li
                        </a>
                        <sup>2</sup>
                    </li>
                    <br>
                    <a></a><br>
                    <li>
                        <sup>1</sup>
                        <a style="font-size: 16px;">
                            Tsinghua Unviersity
                        </a>
                    </li>
                    <li>
                        <sup>2</sup>
                        <a style="font-size: 16px;">
                            Noah's Ark Lab, Huawei
                        </a>
                    </li>
                    <li>
                        <sup>3</sup>
                        <a style="font-size: 16px;">
                            The University of Hong Kong
                        </a>
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
            <div class="col-md-4 col-md-offset-4 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://arxiv.org/abs/2401.15688">
                            <img src="./files/paper.png" height="60px">
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>

 <!--                      <li>
                       <a onClick="alert('Code coming soon!\nContact dengyu2008@hotmail.com for more details.')"> 
                       <a href="https://github.com/microsoft/GRAM">
                            <img src="./files/github.png" height="60px">
                            <h4><strong>Code</strong></h4>
                        </a>
                    </li>-->
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <a>
                    <video style="width:100%;height:100%;" playsinline autoplay loop preload muted>
                        <source src="./files/demo_short.mp4" type="video/mp4">
                    </video>
                </a>
                <br></br>
                <h2>
                    Abstract
                </h2>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    Despite significant advancements in text-to-image models for generating high-quality images, these methods still struggle to ensure the controllability of text prompts over images in the context of complex text prompts, especially when it comes to retaining object attributes and relationships. In this paper, we propose CompAgent, a training-free approach for compositional text-to-image generation, with a large language model (LLM) agent as its core. The fundamental idea underlying CompAgent is premised on a divide-and-conquer methodology. Given a complex text prompt containing multiple concepts including objects, attributes, and relationships, the LLM agent initially decomposes it, which entails the extraction of individual objects, their associated attributes, and the prediction of a coherent scene layout. These individual objects can then be independently conquered. Subsequently, the agent performs reasoning by analyzing the text, plans and employs the tools to compose these isolated objects. The verification and human feedback mechanism is finally incorporated into our agent to further correct the potential attribute errors and refine the generated images. Guided by the LLM agent, we propose a tuning-free multi-concept customization model and a layout-to-image generation model as the tools for concept composition, and a local image editing method as the tool to interact with the agent for verification. The scene layout controls the image generation process among these tools to prevent confusion among multiple objects. 
                    Extensive experiments demonstrate the superiority of our approach for compositional text-to-image generation: CompAgent achieves more than 10\% improvement on T2I-CompBench, a comprehensive benchmark for open-world compositional T2I generation. The extension to various related tasks also illustrates the flexibility of our CompAgent for potential applications.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Video
                </h2>
                <hr style="margin-top:0px">
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/3nukF4M4Zx8" allowfullscreen=""
                            style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Overview
                </h2>
                <hr style="margin-top:0px">
                <img src="./files/framework.jpeg" class="img-responsive" alt="overview"><br>
                <p class="text-justify" style="font-size: 16px;">
                    Overview of CompAgent: Given the input containing the complex text prompt, the LLM agent conducts the decomposition and planning tasks to invoke external tools for image generation. It then performs verification or involves human feedback and interacts with the tools for image self-correction. The final image output will well satisfy the requirements from the input text prompt.
                </p>
            </div>
        </div>



        <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Generation Results
                </h2>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    CompAgent is able to generate accurate images given complex text prompts. For compositional-text-to-image generation, it well addresses the attribute binding and object relationship problems.
                    Compared with existing methods, CompAgent generates images that better align with the description of complex text prompts.
                </p>
                <!-- <video style="width:100%;height:100%;" playsinline autoplay loop preload muted>
                    <source src="./files/result.mp4" type="video/mp4">
                </video> -->
                <div class="col-md-12 col-md-offset-1 text-center">
                    <img src="./files/comparison2.png" class="img-responsive" alt="overview"><br>
                </div>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    CompAgent generates accurately for attribute binding:
                </p>
                <!-- <video style="width:100%;height:100%;" playsinline autoplay loop preload muted>
                    <source src="./files/compare.mp4" type="video/mp4">
                </video> -->
                <div class="col-md-15 col-md-offset-1 text-center">
                    <img src="./files/rcolor.png" class="img-responsive" alt="overview"><br>
                </div>
                <p class="text-justify" style="font-size: 16px;">
                    for object relationship:
                </p>
                <div class="col-md-15 col-md-offset-1 text-center">
                    <img src="./files/rrelation.png" class="img-responsive" alt="overview"><br>
                </div>
                <p class="text-justify" style="font-size: 16px;">
                    for complex compositions:
                </p>
                <div class="col-md-15 col-md-offset-1 text-center">
                    <img src="./files/rcomplex.png" class="img-responsive" alt="overview"><br>
                </div>
            </div>
        </div>

        <!-- <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Manifolds Visualization
                </h2>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    GRAM constrains point sampling and radiance field learning on 2D manifolds, embodied as a set of
                    implicit surfaces. These implicit surfaces are shared for the trained object category, jointly
                    learned with GAN training, and fixed at inference time.
                </p>
                <video style="width:93%;height:93%;" playsinline autoplay loop preload muted>
                    <source src="./files/manifold.mp4" type="video/mp4">
                </video>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    3D Geometry Visualization
                </h2>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    Although GRAM confines the input domain of the radiance field on 2D manifolds, we can still extract
                    proxy 3D shapes of the generated objects using the volume-based marching cubes algorithm. It can be
                    observed that GRAM produces high-quality geometry with detailed structures well depicted, which is
                    the key to achieve strong visual
                    3D consistency across different views.
                </p>
                <video style="width:100%;height:100%;" playsinline autoplay loop preload muted>
                    <source src="./files/shape.mp4" type="video/mp4">
                </video>
            </div>
        </div> -->

<!--         <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Image Embedding and Editing
                </h2>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                GAN inversion is naturally supported by GRAM. Given an input image, we can first embed it into the learned latent space and then freely move the camera viewpoint to synthesize images at novel views.
                </p>
                <video style="width:67%;height:67%;" playsinline autoplay loop preload muted>
                    <source src="./files/inv2.mp4" type="video/mp4">
                </video>
            </div>
        </div> -->

        <!-- <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Responsible AI Considerations
                </h2>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    The goal of this paper is to study generative modelling of the 3D objects from 2D images, 
                    and to provide a method for generating multi-view images of non-existing, virtual objects. 
                    It is not intended to manipulate existing images nor to create content that is used to mislead or deceive. 
                    This method does not have understanding and control of the generated content. 
                    Thus, adding targeted facial expressions or mouth movements is out of the scope of this work. 
                    However, the method, like all other related AI image generation techniques, 
                    could still potentially be misused for impersonating humans. Currently, 
                    the images generated by this method contain visual artifacts, unnatural texture patterns, 
                    and other unpredictable failures that can be spotted by humans and fake image detection algorithms. 
                    We also plan to investigate applying this technology for advancing 3D- and video-based forgery detection.  
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Availability of Software
                </h2>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    Per concerns about misuse of this method, the code is available for use under a <a href="https://github.com/microsoft/GRAM/blob/main/GRAM-Microsoft%20Research%20License%20Agreement.pdf">research-only license</a>. 
                </p>
            </div>
        </div> -->
        
        <section class="section" id="BibTeX">
            <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@article{wang2024div,
            author    = {Zhenyu, Wang and Enze, Xie and Aoxue, Li and Zhongdao, Wang and Xihui, Liu and Zhenguo, Li},
            title     = {Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation},
            journal   = {arXiv preprint arXiv:2401.15688},
            year      = {2024},
        }</code></pre>
            </div>
        </section>

        <!-- <div class="row">
            <div class="col-md-12 col-md-offset-0">
                <div class="text-center">
                    <h2>
                        Citation
                    </h2>
                </div>
                <hr style="margin-top:0px">
                <div class="form-group col-md-12 col-md-offset-0">
                    <div class="CodeMirror cm-s-default CodeMirror-wrap" style="font-size: 16px;">
                        <div
                            style="overflow: hidden; position: relative; width: 3px; height: 0px; top: 4px; left: 4px; ">
                            <textarea autocorrect="off" autocapitalize="off" spellcheck="false"
                                style="position: absolute; padding: 0px; width: 1000px; height: 1em; outline: none;"
                                tabindex="0"></textarea></div>
                        <div class="CodeMirror-vscrollbar" cm-not-content="true">
                            <div style="min-width: 1px; height: 0px;"></div>
                        </div>
                        <div class="CodeMirror-hscrollbar" cm-not-content="true">
                            <div style="height: 100%; min-height: 1px; width: 0px;"></div>
                        </div>
                        <div class="CodeMirror-scrollbar-filler" cm-not-content="true"></div>
                        <div class="CodeMirror-gutter-filler" cm-not-content="true"></div>
                        <div class="CodeMirror-scroll" tabindex="-1">
                            <div class="CodeMirror-sizer"
                                style="margin-left: 0px; margin-bottom: -17px; border-right-width: 13px; min-height: 162px; padding-right: 0px; padding-bottom: 0px;">
                                <div style="position: relative; top: 0px;">
                                    <div class="CodeMirror-lines">
                                        <div style="position: relative; outline: none;">
                                            <div class="CodeMirror-measure">AخA</div>
                                            <div class="CodeMirror-measure"></div>
                                            <div style="position: relative; z-index: 1;"></div>
                                            <div class="CodeMirror-cursors">
                                                <div class="CodeMirror-cursor"
                                                    style="left: 4px; top: 0px; height: 17.1406px;">&nbsp;</div>
                                            </div>
                                            <div class="CodeMirror-code" style="">
                                                <pre
                                                    class=" CodeMirror-line "><span style="padding-right: 0.1px;">@article{wang2024div,</span></pre>
                                                <pre
                                                    class=" CodeMirror-line "><span style="padding-right: 0.1px;"> &nbsp;  title={Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation},</span></pre>
                                                <pre
                                                    class=" CodeMirror-line "><span style="padding-right: 0.1px;"> &nbsp;  author={Zhenyu, Wang and Enze, Xie and Aoxue, Li and Zhongdao, Wang and Xihui, Liu and Zhenguo, Li},</span></pre>
                                                <pre
                                                    class=" CodeMirror-line "><span style="padding-right: 0.1px;"> &nbsp;  journal={***},</span></pre>
                                                <pre
                                                    class=" CodeMirror-line "><span style="padding-right: 0.1px;"> &nbsp;  year={2024}</span></pre>
                                                <pre
                                                    class=" CodeMirror-line "><span style="padding-right: 0.1px;">}</span></pre>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div style="position: absolute; height: 13px; width: 1px; top: 280px;"></div>
                            <div class="CodeMirror-gutters" style="display: none; height: 300px;"></div>
                        </div>
                    </div>
                </div>
            </div>
        </div> -->

        <!-- <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Acknowledgements
                </h2>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    We thank Harry Shum for the fruitful advice and discussion to improve the paper. <br>
                    The website template was adapted from <a href="https://jonbarron.info/mipnerf/">Mip-NeRF</a>.
                </p>
            </div>
        </div> -->


</body>

</html>
